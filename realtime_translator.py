#!/usr/bin/env python3
"""
å®æ—¶è¯­éŸ³ç¿»è¯‘å­—å¹•ç³»ç»Ÿ

åŠŸèƒ½:
1. å®æ—¶é‡‡é›†ç”µè„‘éŸ³é¢‘è¾“å‡º (PulseAudio)
2. ä½¿ç”¨ OpenAI Whisper è¿›è¡Œè¯­éŸ³è¯†åˆ«
3. å®æ—¶ç¿»è¯‘å¹¶æ˜¾ç¤ºå­—å¹•

ä¾èµ–:
    pip install -r requirements.txt

ä½¿ç”¨:
    python realtime_translator.py --help
"""

import sys
import time
import threading
import queue
import argparse
import tempfile
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Callable

import numpy as np
import torch

# éŸ³é¢‘å¤„ç†
import soundfile as sf

# è¯­éŸ³è¯†åˆ«
import whisper

# ç¿»è¯‘
from deep_translator import GoogleTranslator, DeepLTranslator

# ä»åŒç›®å½•å¯¼å…¥éŸ³é¢‘é‡‡é›†æ¨¡å—
from audio_capture import PulseAudioCapture, AudioChunkBuffer


class RealtimeTranslator:
    """
    å®æ—¶ç¿»è¯‘å­—å¹•ç³»ç»Ÿä¸»ç±»
    """
    
    def __init__(
        self,
        whisper_model: str = "base",
        source_language: str = "en",
        target_language: str = "zh",
        min_audio_duration: float = 2.0,
        max_audio_duration: float = 10.0,
        silence_threshold: float = 0.02,
        silence_duration: float = 1.5,
        output_file: Optional[str] = None,
        source_name: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨
        
        Args:
            whisper_model: Whisper æ¨¡å‹å¤§å° ("tiny", "base", "small", "medium", "large")
            source_language: æºè¯­è¨€ä»£ç  (å¦‚ "en", "zh", "ja", "ko")
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            min_audio_duration: æœ€å°éŸ³é¢‘æ—¶é•¿ (ç§’) - ç”¨äºè§¦å‘è¯†åˆ«
            max_audio_duration: æœ€å¤§éŸ³é¢‘æ—¶é•¿ (ç§’)
            silence_threshold: é™éŸ³é˜ˆå€¼ (ç”¨äºæ£€æµ‹è¯­éŸ³ç»“æŸ)
            silence_duration: é™éŸ³ç­‰å¾…æ—¶é•¿ (ç§’)
            output_file: è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„
            source_name: PulseAudio monitor source åç§°
        """
        self.whisper_model = whisper_model
        self.source_language = source_language
        self.target_language = target_language
        self.min_audio_duration = min_audio_duration
        self.max_audio_duration = max_audio_duration
        self.silence_threshold = silence_threshold
        self.silence_duration = silence_duration
        self.output_file = output_file
        self.source_name = source_name
        
        # çŠ¶æ€
        self._running = False
        self._whisper_model = None
        self._translator = None
        
        # éŸ³é¢‘é‡‡é›†
        self._audio_capture: Optional[PulseAudioCapture] = None
        self._audio_buffer = AudioChunkBuffer(
            sample_rate=16000,
            min_duration=min_audio_duration,
            max_duration=max_audio_duration
        )
        
        # å­—å¹•å›è°ƒ
        self._subtitle_callbacks: List[Callable] = []
        
        # è¯­éŸ³çŠ¶æ€
        self._is_speaking = False
        self._last_speech_time = time.time()
        
        # çº¿ç¨‹
        self._recognition_thread: Optional[threading.Thread] = None
        self._recognition_queue = queue.Queue()
        
        # ç¿»è¯‘å™¨åˆå§‹åŒ–
        self._init_translator()
    
    def _init_translator(self):
        """åˆå§‹åŒ–ç¿»è¯‘å™¨"""
        try:
            self._translator = GoogleTranslator(
                source=self.source_language,
                target=self.target_language
            )
        except Exception as e:
            print(f"[WARNING] Google Translator åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                self._translator = DeepLTranslator(
                    source=self.source_language.upper(),
                    target=self.target_language.upper()
                )
            except Exception as e2:
                print(f"[WARNING] DeepL Translator ä¹Ÿå¤±è´¥: {e2}")
                self._translator = None
    
    def _load_whisper_model(self):
        """åŠ è½½ Whisper æ¨¡å‹"""
        print(f"[INFO] åŠ è½½ Whisper æ¨¡å‹: {self.whisper_model}")
        
        # æ£€æŸ¥ CUDA
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[INFO] ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        self._whisper_model = whisper.load_model(
            self.whisper_model,
            device=device
        )
        
        print("[INFO] Whisper æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def add_subtitle_callback(self, callback: Callable[[str, str, float], None]):
        """
        æ·»åŠ å­—å¹•å›è°ƒå‡½æ•°
        
        Args:
            callback: æ¥æ”¶ (åŸå§‹æ–‡æœ¬, ç¿»è¯‘æ–‡æœ¬, æ—¶é—´æˆ³) çš„å‡½æ•°
        """
        self._subtitle_callbacks.append(callback)
    
    def _emit_subtitle(self, original: str, translated: str):
        """å‘é€å­—å¹•åˆ°æ‰€æœ‰å›è°ƒ"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        for callback in self._subtitle_callbacks:
            try:
                callback(original, translated, timestamp)
            except Exception as e:
                print(f"[WARNING] å­—å¹•å›è°ƒå¤±è´¥: {e}")
        
        # å†™å…¥æ–‡ä»¶
        if self.output_file:
            self._write_subtitle_to_file(original, translated, timestamp)
    
    def _write_subtitle_to_file(self, original: str, translated: str, timestamp: str):
        """å†™å…¥å­—å¹•åˆ°æ–‡ä»¶"""
        try:
            with open(self.output_file, "a", encoding="utf-8") as f:
                f.write(f"[{timestamp}] {original}\n")
                f.write(f"[{timestamp}] ç¿»è¯‘: {translated}\n\n")
        except Exception as e:
            print(f"[WARNING] å†™å…¥å­—å¹•æ–‡ä»¶å¤±è´¥: {e}")
    
    def _detect_speech_activity(self, audio_chunk: np.ndarray) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰è¯­éŸ³æ´»åŠ¨"""
        rms = np.sqrt(np.mean(audio_chunk**2))
        return rms > self.silence_threshold
    
    def _process_audio(self):
        """å¤„ç†éŸ³é¢‘çš„çº¿ç¨‹å‡½æ•°"""
        print("[INFO] éŸ³é¢‘å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
        
        temp_dir = tempfile.mkdtemp()
        
        while self._running:
            try:
                # è¯»å–éŸ³é¢‘å—
                audio_chunk = self._audio_capture.read(timeout=0.5)
                
                if audio_chunk is None:
                    continue
                
                # æ£€æµ‹è¯­éŸ³æ´»åŠ¨
                has_speech = self._detect_speech_activity(audio_chunk)
                current_time = time.time()
                
                if has_speech:
                    self._is_speaking = True
                    self._last_speech_time = current_time
                    self._audio_buffer.add(audio_chunk)
                else:
                    # æ£€æµ‹è¯­éŸ³æ˜¯å¦ç»“æŸ (é™éŸ³è¶…è¿‡é˜ˆå€¼)
                    if self._is_speaking:
                        silence_elapsed = current_time - self._last_speech_time
                        
                        if silence_elapsed >= self.silence_duration:
                            # è¯­éŸ³ç»“æŸï¼Œå¤„ç†ç´¯ç§¯çš„éŸ³é¢‘
                            self._is_speaking = False
                            
                            if self._audio_buffer.has_enough_audio():
                                audio = self._audio_buffer.get_audio()
                                if audio is not None and len(audio) > 0:
                                    # å‘é€åˆ°è¯†åˆ«é˜Ÿåˆ—
                                    self._recognition_queue.put(audio.copy())
                            
                            self._audio_buffer.clear()
                
                # å¼ºåˆ¶å¤„ç†è¶…é•¿éŸ³é¢‘
                if self._is_speaking and self._audio_buffer.has_enough_audio():
                    audio = self._audio_buffer.get_audio()
                    if audio is not None:
                        audio_duration = len(audio) / 16000
                        if audio_duration >= self.max_audio_duration:
                            self._recognition_queue.put(audio.copy())
                            self._audio_buffer.clear()
                
            except Exception as e:
                if self._running:
                    print(f"[ERROR] éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        print("[INFO] éŸ³é¢‘å¤„ç†çº¿ç¨‹å·²åœæ­¢")
    
    def _recognize_and_translate(self):
        """è¯†åˆ«å’Œç¿»è¯‘çš„çº¿ç¨‹å‡½æ•°"""
        print("[INFO] è¯†åˆ«ç¿»è¯‘çº¿ç¨‹å·²å¯åŠ¨")
        
        while self._running:
            try:
                # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘
                audio = self._recognition_queue.get(timeout=1.0)
                
                if audio is None:
                    continue
                
                # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                temp_dir = tempfile.gettempdir()
                temp_file = Path(temp_dir) / f"audio_{int(time.time() * 1000)}.wav"
                
                try:
                    # ä¿å­˜éŸ³é¢‘
                    sf.write(str(temp_file), audio, 16000)
                    
                    # è¯­éŸ³è¯†åˆ«
                    result = self._whisper_model.transcribe(
                        str(temp_file),
                        language=self.source_language,
                        fp16=False
                    )
                    
                    text = result["text"].strip()
                    
                    if text and len(text) > 3:  # è¿‡æ»¤å¤ªçŸ­çš„è¯†åˆ«ç»“æœ
                        print(f"\nğŸ¤ è¯†åˆ«: {text}")
                        
                        # ç¿»è¯‘
                        if self._translator:
                            try:
                                translated = self._translator.translate(text)
                                print(f"ğŸŒ ç¿»è¯‘: {translated}")
                            except Exception as e:
                                print(f"[WARNING] ç¿»è¯‘å¤±è´¥: {e}")
                                translated = "[ç¿»è¯‘å¤±è´¥]"
                        else:
                            translated = "[ç¿»è¯‘å™¨æœªåˆå§‹åŒ–]"
                        
                        # å‘é€å­—å¹•
                        self._emit_subtitle(text, translated)
                
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        temp_file.unlink()
                    except:
                        pass
                    
                    self._recognition_queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                if self._running:
                    print(f"[ERROR] è¯†åˆ«ç¿»è¯‘é”™è¯¯: {e}")
        
        print("[INFO] è¯†åˆ«ç¿»è¯‘çº¿ç¨‹å·²åœæ­¢")
    
    def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        if self._running:
            print("[WARNING] ç³»ç»Ÿå·²åœ¨è¿è¡Œä¸­")
            return
        
        print("=" * 60)
        print("ğŸ™ï¸  å®æ—¶è¯­éŸ³ç¿»è¯‘å­—å¹•ç³»ç»Ÿ")
        print("=" * 60)
        
        # åŠ è½½æ¨¡å‹
        self._load_whisper_model()
        
        # åˆå§‹åŒ–éŸ³é¢‘é‡‡é›†
        self._audio_capture = PulseAudioCapture(
            source_name=self.source_name,
            sample_rate=16000,
            channels=1,
            blocksize=3200
        )
        
        # åˆ—å‡ºéŸ³é¢‘è®¾å¤‡
        self._audio_capture.list_available_sinks()
        
        # å¯åŠ¨éŸ³é¢‘é‡‡é›†
        self._audio_capture.start()
        
        # è®¾ç½®è¿è¡ŒçŠ¶æ€
        self._running = True
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self._recognition_thread = threading.Thread(
            target=self._process_and_recognize,
            daemon=True
        )
        self._recognition_thread.start()
        
        print("\n[INFO] ç³»ç»Ÿå·²å¯åŠ¨!")
        print("[INFO] æ­£åœ¨ç›‘å¬éŸ³é¢‘... (æŒ‰ Ctrl+C åœæ­¢)")
        print("-" * 60)
    
    def _process_and_recognize(self):
        """å¤„ç†å’Œè¯†åˆ«çš„ä¸»å¾ªç¯"""
        temp_dir = tempfile.mkdtemp()
        
        while self._running:
            try:
                # è¯»å–éŸ³é¢‘å—
                audio_chunk = self._audio_capture.read(timeout=0.5)
                
                if audio_chunk is None:
                    continue
                
                # æ£€æµ‹è¯­éŸ³æ´»åŠ¨
                has_speech = self._detect_speech_activity(audio_chunk)
                current_time = time.time()
                
                if has_speech:
                    self._is_speaking = True
                    self._last_speech_time = current_time
                    self._audio_buffer.add(audio_chunk)
                else:
                    # æ£€æµ‹è¯­éŸ³ç»“æŸ
                    if self._is_speaking:
                        silence_elapsed = current_time - self._last_speech_time
                        
                        if silence_elapsed >= self.silence_duration:
                            self._is_speaking = False
                            
                            if self._audio_buffer.has_enough_audio():
                                audio = self._audio_buffer.get_audio()
                                if audio is not None and len(audio) > 0:
                                    self._recognize_audio(audio, temp_dir)
                            
                            self._audio_buffer.clear()
                
                # å¤„ç†è¶…é•¿éŸ³é¢‘
                if self._is_speaking and self._audio_buffer.has_enough_audio():
                    audio = self._audio_buffer.get_audio()
                    if audio is not None:
                        audio_duration = len(audio) / 16000
                        if audio_duration >= self.max_audio_duration:
                            self._recognize_audio(audio, temp_dir)
                            self._audio_buffer.clear()
                
            except Exception as e:
                if self._running:
                    print(f"[ERROR] å¤„ç†é”™è¯¯: {e}")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _recognize_audio(self, audio: np.ndarray, temp_dir: str):
        """è¯†åˆ«å•ä¸ªéŸ³é¢‘æ®µ"""
        temp_file = Path(temp_dir) / f"audio_{int(time.time() * 1000)}.wav"
        
        try:
            sf.write(str(temp_file), audio, 16000)
            
            result = self._whisper_model.transcribe(
                str(temp_file),
                language=self.source_language,
                fp16=False
            )
            
            text = result["text"].strip()
            
            if text and len(text) > 2:
                print(f"\nğŸ¤ [{datetime.now().strftime('%H:%M:%S')}] {text}")
                
                if self._translator:
                    try:
                        translated = self._translator.translate(text)
                        print(f"ğŸŒ ç¿»è¯‘: {translated}")
                    except Exception as e:
                        print(f"[WARNING] ç¿»è¯‘å¤±è´¥: {e}")
                        translated = "[ç¿»è¯‘å¤±è´¥]"
                else:
                    translated = "[ç¿»è¯‘å™¨æœªåˆå§‹åŒ–]"
                
                self._emit_subtitle(text, translated)
        
        except Exception as e:
            print(f"[ERROR] è¯†åˆ«é”™è¯¯: {e}")
        
        finally:
            try:
                temp_file.unlink()
            except:
                pass
    
    def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        if not self._running:
            return
        
        print("\n[INFO] æ­£åœ¨åœæ­¢ç³»ç»Ÿ...")
        self._running = False
        
        # åœæ­¢éŸ³é¢‘é‡‡é›†
        if self._audio_capture:
            self._audio_capture.stop()
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self._recognition_thread:
            self._recognition_thread.join(timeout=5.0)
        
        # æ¸…ç†æ¨¡å‹
        if self._whisper_model:
            del self._whisper_model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        print("[INFO] ç³»ç»Ÿå·²åœæ­¢")
    
    def run(self):
        """ä¸»è¿è¡Œå¾ªç¯ (é˜»å¡)"""
        try:
            self.start()
            
            # ä¸»å¾ªç¯
            while self._running:
                time.sleep(0.1)
                
        except KeyboardInterrupt:
            print("\n[INFO] æ”¶åˆ°ä¸­æ–­ä¿¡å·")
        finally:
            self.stop()


def console_subtitle_callback(original: str, translated: str, timestamp: str):
    """æ§åˆ¶å°å­—å¹•æ˜¾ç¤ºå›è°ƒ"""
    print(f"\n{'='*60}")
    print(f"â° {timestamp}")
    print(f"ğŸ“ åŸæ–‡: {original}")
    print(f"ğŸŒ è¯‘æ–‡: {translated}")
    print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(
        description="å®æ—¶è¯­éŸ³ç¿»è¯‘å­—å¹•ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤è®¾ç½®è¿è¡Œ
  python realtime_translator.py
  
  # ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (æ›´å‡†ç¡®ä½†æ›´æ…¢)
  python realtime_translator.py --model medium
  
  # ç¿»è¯‘æ—¥è¯­åˆ°ä¸­æ–‡
  python realtime_translator.py --source ja --target zh
  
  # ä¿å­˜å­—å¹•åˆ°æ–‡ä»¶
  python realtime_translator.py --output subtitles.srt
  
  # æŸ¥çœ‹å¯ç”¨çš„éŸ³é¢‘æº
  python realtime_translator.py --list-sources
        """
    )
    
    parser.add_argument(
        "--model", "-m",
        default="base",
        choices=["tiny", "base", "small", "medium", "large"],
        help="Whisper æ¨¡å‹å¤§å° (é»˜è®¤: base)"
    )
    
    parser.add_argument(
        "--source", "-s",
        default="en",
        help="æºè¯­è¨€ä»£ç  (é»˜è®¤: en)"
    )
    
    parser.add_argument(
        "--target", "-t",
        default="zh",
        help="ç›®æ ‡è¯­è¨€ä»£ç  (é»˜è®¤: zh)"
    )
    
    parser.add_argument(
        "--output", "-o",
        default=None,
        help="è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--source-name",
        default=None,
        help="PulseAudio monitor source åç§°"
    )
    
    parser.add_argument(
        "--list-sources",
        action="store_true",
        help="åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘æºå¹¶é€€å‡º"
    )
    
    parser.add_argument(
        "--silence-threshold",
        type=float,
        default=0.02,
        help="é™éŸ³æ£€æµ‹é˜ˆå€¼ (é»˜è®¤: 0.02)"
    )
    
    parser.add_argument(
        "--silence-duration",
        type=float,
        default=1.5,
        help="é™éŸ³ç­‰å¾…æ—¶é•¿ (ç§’) (é»˜è®¤: 1.5)"
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºéŸ³é¢‘æº
    if args.list_sources:
        capture = PulseAudioCapture()
        capture.list_available_sources()
        capture.list_available_sinks()
        return
    
    # åˆ›å»ºç¿»è¯‘å™¨
    translator = RealtimeTranslator(
        whisper_model=args.model,
        source_language=args.source,
        target_language=args.target,
        silence_threshold=args.silence_threshold,
        silence_duration=args.silence_duration,
        output_file=args.output,
        source_name=args.source_name
    )
    
    # æ·»åŠ æ§åˆ¶å°è¾“å‡ºå›è°ƒ
    translator.add_subtitle_callback(console_subtitle_callback)
    
    # è¿è¡Œ
    translator.run()


if __name__ == "__main__":
    main()
